{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d91457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.data_collection.paper_collector import ArXivCollector, Paper\n",
    "from src.analysis.paper_analyzer import PaperAnalyzer, PaperAnalysis\n",
    "from src.analysis.knowledge_extractor import KnowledgeExtractor, ResearchGap, Hypothesis\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a28011b",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load papers, analyses, and knowledge extraction results from a pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d5bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your data directory and run name\n",
    "DATA_DIR = Path(\"../data/results\")\n",
    "RUN_NAME = \"graphene_thermal_conduc_20241231_120000\"  # Change to your run name\n",
    "\n",
    "# Or list available runs\n",
    "if DATA_DIR.exists():\n",
    "    available_runs = [f.stem.replace('_SUMMARY', '') for f in DATA_DIR.glob('*_SUMMARY.json')]\n",
    "    if available_runs:\n",
    "        print(\"Available runs:\")\n",
    "        for run in available_runs:\n",
    "            print(f\"  - {run}\")\n",
    "        RUN_NAME = available_runs[-1]  # Use most recent\n",
    "        print(f\"\\nUsing: {RUN_NAME}\")\n",
    "    else:\n",
    "        print(\"No runs found. Run collect_and_analyze.py first!\")\n",
    "else:\n",
    "    print(f\"Data directory not found: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a491795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load papers\n",
    "collector = ArXivCollector()\n",
    "papers = collector.load_papers(f\"{RUN_NAME}_papers\")\n",
    "\n",
    "print(f\"ðŸ“š Loaded {len(papers)} papers\")\n",
    "print(f\"   First paper: {papers[0].title[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec354024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analyses\n",
    "analyzer = PaperAnalyzer()\n",
    "analyses = analyzer.load_analyses(f\"{RUN_NAME}_analyses\")\n",
    "\n",
    "print(f\"ðŸ¤– Loaded {len(analyses)} analyses\")\n",
    "print(f\"   Avg relevance: {np.mean([a.relevance_score for a in analyses]):.2f}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load knowledge graph\n",
    "extractor = KnowledgeExtractor()\n",
    "graph = extractor.load_knowledge_graph(f\"{RUN_NAME}_knowledge_graph\")\n",
    "\n",
    "stats = extractor.get_graph_statistics()\n",
    "print(f\"ðŸ§  Knowledge Graph:\")\n",
    "print(f\"   {stats['total_nodes']} nodes, {stats['total_edges']} edges\")\n",
    "print(f\"   {stats['num_materials']} materials, {stats['num_properties']} properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7719662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gaps and hypotheses\n",
    "with open(DATA_DIR / f\"{RUN_NAME}_gaps.json\", 'r', encoding='utf-8') as f:\n",
    "    gaps_data = json.load(f)\n",
    "    gaps = [ResearchGap(**g) for g in gaps_data]\n",
    "\n",
    "with open(DATA_DIR / f\"{RUN_NAME}_hypotheses.json\", 'r', encoding='utf-8') as f:\n",
    "    hyp_data = json.load(f)\n",
    "    hypotheses = [Hypothesis(**h) for h in hyp_data]\n",
    "\n",
    "print(f\"ðŸŽ¯ Loaded {len(gaps)} gaps and {len(hypotheses)} hypotheses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56927f68",
   "metadata": {},
   "source": [
    "## 2. Explore Papers\n",
    "\n",
    "Analyze the collected papers: publication dates, authors, categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d3b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "papers_df = collector.get_papers_dataframe(papers)\n",
    "\n",
    "print(f\"Papers DataFrame shape: {papers_df.shape}\")\n",
    "papers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fbc20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication timeline\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "papers_df['published_date'].dt.date.value_counts().sort_index().plot(kind='bar', ax=ax)\n",
    "ax.set_title('Papers by Publication Date', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Number of Papers')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97508abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top categories\n",
    "all_categories = []\n",
    "for cats in papers_df['categories']:\n",
    "    all_categories.extend(cats)\n",
    "\n",
    "cat_counts = Counter(all_categories)\n",
    "top_cats = dict(cat_counts.most_common(10))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plt.barh(list(top_cats.keys()), list(top_cats.values()))\n",
    "ax.set_title('Top 10 arXiv Categories', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Number of Papers')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total unique categories: {len(cat_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed0f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top authors\n",
    "all_authors = []\n",
    "for authors in papers_df['authors']:\n",
    "    all_authors.extend(authors)\n",
    "\n",
    "author_counts = Counter(all_authors)\n",
    "top_authors = dict(author_counts.most_common(15))\n",
    "\n",
    "print(\"Top 15 Most Prolific Authors:\")\n",
    "for i, (author, count) in enumerate(top_authors.items(), 1):\n",
    "    print(f\"{i:2d}. {author:40s} ({count} papers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cbe0b0",
   "metadata": {},
   "source": [
    "## 3. Analyze Paper Analyses\n",
    "\n",
    "Explore the AI-generated analyses: relevance scores, research types, extracted entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc7c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "analyses_df = analyzer.get_analysis_dataframe(analyses)\n",
    "\n",
    "print(f\"Analyses DataFrame shape: {analyses_df.shape}\")\n",
    "analyses_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59320200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevance score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(analyses_df['relevance_score'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(analyses_df['relevance_score'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {analyses_df[\"relevance_score\"].mean():.2f}')\n",
    "axes[0].set_title('Relevance Score Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Relevance Score (0-10)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "analyses_df.boxplot(column='relevance_score', ax=axes[1])\n",
    "axes[1].set_title('Relevance Score Box Plot', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Relevance Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Relevance Statistics:\")\n",
    "print(analyses_df['relevance_score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c659e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research type distribution\n",
    "type_counts = analyses_df['research_type'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "type_counts.plot(kind='pie', ax=ax, autopct='%1.1f%%', startangle=90)\n",
    "ax.set_title('Research Type Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739bffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maturity level distribution\n",
    "maturity_counts = analyses_df['maturity_level'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "maturity_counts.plot(kind='barh', ax=ax)\n",
    "ax.set_title('Research Maturity Level', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Number of Papers')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a3786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top papers by relevance\n",
    "print(\"\\nðŸŒŸ Top 10 Most Relevant Papers:\\n\")\n",
    "top_papers = analyses_df.nlargest(10, 'relevance_score')\n",
    "\n",
    "for i, row in top_papers.iterrows():\n",
    "    print(f\"{i+1}. [{row['relevance_score']:.1f}/10] {row['title'][:80]}...\")\n",
    "    print(f\"   Type: {row['research_type']}, Materials: {row['materials'][:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity extraction statistics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(analyses_df['num_materials'], bins=15, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Materials per Paper')\n",
    "axes[0].set_xlabel('Number of Materials')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(analyses_df['num_properties'], bins=15, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_title('Properties per Paper')\n",
    "axes[1].set_xlabel('Number of Properties')\n",
    "\n",
    "axes[2].hist(analyses_df['num_methods'], bins=15, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[2].set_title('Methods per Paper')\n",
    "axes[2].set_xlabel('Number of Methods')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fa8251",
   "metadata": {},
   "source": [
    "## 4. Visualize Knowledge Graph\n",
    "\n",
    "Explore the knowledge graph structure and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08730951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph statistics\n",
    "print(\"Knowledge Graph Statistics:\")\n",
    "print(f\"  Nodes: {graph.number_of_nodes()}\")\n",
    "print(f\"  Edges: {graph.number_of_edges()}\")\n",
    "print(f\"  Density: {nx.density(graph):.4f}\")\n",
    "\n",
    "# Node type breakdown\n",
    "node_types = {}\n",
    "for node, data in graph.nodes(data=True):\n",
    "    node_type = data.get('type', 'unknown')\n",
    "    node_types[node_type] = node_types.get(node_type, 0) + 1\n",
    "\n",
    "print(f\"\\nNode Types:\")\n",
    "for ntype, count in sorted(node_types.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {ntype}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df434234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most connected nodes\n",
    "degree_dict = dict(graph.degree())\n",
    "top_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "print(\"Top 20 Most Connected Nodes:\\n\")\n",
    "for i, (node, degree) in enumerate(top_nodes, 1):\n",
    "    node_type = graph.nodes[node].get('type', 'unknown')\n",
    "    freq = graph.nodes[node].get('frequency', 0)\n",
    "    print(f\"{i:2d}. {node:30s} | {node_type:10s} | {degree:3d} connections | {freq:2d} papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22042f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize subgraph of top materials and their properties\n",
    "materials = [n for n, d in graph.nodes(data=True) if d.get('type') == 'material']\n",
    "top_materials = sorted(materials, key=lambda m: graph.nodes[m]['frequency'], reverse=True)[:5]\n",
    "\n",
    "# Get connected nodes\n",
    "subgraph_nodes = set(top_materials)\n",
    "for mat in top_materials:\n",
    "    neighbors = list(graph.neighbors(mat))\n",
    "    subgraph_nodes.update(neighbors[:10])  # Add top 10 neighbors\n",
    "\n",
    "subgraph = graph.subgraph(subgraph_nodes)\n",
    "\n",
    "# Draw graph\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "pos = nx.spring_layout(subgraph, k=2, iterations=50)\n",
    "\n",
    "# Color nodes by type\n",
    "node_colors = []\n",
    "for node in subgraph.nodes():\n",
    "    ntype = subgraph.nodes[node].get('type', 'unknown')\n",
    "    if ntype == 'material':\n",
    "        node_colors.append('lightblue')\n",
    "    elif ntype == 'property':\n",
    "        node_colors.append('lightcoral')\n",
    "    elif ntype == 'method':\n",
    "        node_colors.append('lightgreen')\n",
    "    else:\n",
    "        node_colors.append('gray')\n",
    "\n",
    "# Draw\n",
    "nx.draw_networkx_nodes(subgraph, pos, node_color=node_colors, node_size=500, alpha=0.8, ax=ax)\n",
    "nx.draw_networkx_edges(subgraph, pos, alpha=0.2, ax=ax)\n",
    "nx.draw_networkx_labels(subgraph, pos, font_size=8, ax=ax)\n",
    "\n",
    "ax.set_title(f'Knowledge Graph: Top {len(top_materials)} Materials and Connected Entities', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='lightblue', label='Materials'),\n",
    "    Patch(facecolor='lightcoral', label='Properties'),\n",
    "    Patch(facecolor='lightgreen', label='Methods')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8cb8fa",
   "metadata": {},
   "source": [
    "## 5. Research Patterns\n",
    "\n",
    "Analyze frequent patterns and co-occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474cc51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find patterns\n",
    "patterns = extractor.find_frequent_patterns(min_frequency=2)\n",
    "\n",
    "print(\"Top Materials:\")\n",
    "for i, (mat, freq) in enumerate(patterns['top_materials'][:10], 1):\n",
    "    print(f\"{i:2d}. {mat:40s} ({freq} mentions)\")\n",
    "\n",
    "print(\"\\nTop Properties:\")\n",
    "for i, (prop, freq) in enumerate(patterns['top_properties'][:10], 1):\n",
    "    print(f\"{i:2d}. {prop:40s} ({freq} mentions)\")\n",
    "\n",
    "print(\"\\nTop Methods:\")\n",
    "for i, (method, freq) in enumerate(patterns['top_methods'][:10], 1):\n",
    "    print(f\"{i:2d}. {method:40s} ({freq} mentions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5619ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Material-Property co-occurrence\n",
    "print(\"\\nTop Material-Property Pairs:\")\n",
    "for i, (pair, freq) in enumerate(patterns['material_property_pairs'][:15], 1):\n",
    "    print(f\"{i:2d}. {pair:60s} ({freq} co-occurrences)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f78dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top materials\n",
    "if patterns['top_materials']:\n",
    "    mats, freqs = zip(*patterns['top_materials'][:10])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    plt.barh(mats, freqs)\n",
    "    ax.set_title('Top 10 Materials by Frequency', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ab9b5e",
   "metadata": {},
   "source": [
    "## 6. Research Gaps\n",
    "\n",
    "Explore identified research gaps and their priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01df1d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Research Gaps Identified: {len(gaps)}\\n\")\n",
    "\n",
    "# Sort by priority and confidence\n",
    "priority_order = {'high': 3, 'medium': 2, 'low': 1}\n",
    "sorted_gaps = sorted(gaps, key=lambda g: (priority_order.get(g.priority, 0), g.confidence), reverse=True)\n",
    "\n",
    "print(\"Top Research Gaps:\\n\")\n",
    "for i, gap in enumerate(sorted_gaps[:10], 1):\n",
    "    print(f\"{i}. [{gap.priority.upper()} priority, {gap.confidence:.0%} confidence]\")\n",
    "    print(f\"   {gap.description}\")\n",
    "    if gap.related_materials:\n",
    "        print(f\"   Materials: {', '.join(gap.related_materials[:5])}\")\n",
    "    if gap.related_properties:\n",
    "        print(f\"   Properties: {', '.join(gap.related_properties[:5])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f8b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gap statistics\n",
    "gap_priorities = Counter(g.priority for g in gaps)\n",
    "gap_confidences = [g.confidence for g in gaps]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Priority distribution\n",
    "axes[0].bar(gap_priorities.keys(), gap_priorities.values())\n",
    "axes[0].set_title('Research Gaps by Priority', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Confidence distribution\n",
    "axes[1].hist(gap_confidences, bins=10, edgecolor='black', alpha=0.7)\n",
    "axes[1].set_title('Gap Confidence Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Confidence')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d704eb9e",
   "metadata": {},
   "source": [
    "## 7. Research Hypotheses\n",
    "\n",
    "Examine AI-generated research hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dffbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total Hypotheses Generated: {len(hypotheses)}\\n\")\n",
    "\n",
    "# Sort by novelty and feasibility\n",
    "feasibility_order = {'high': 3, 'medium': 2, 'low': 1}\n",
    "sorted_hyp = sorted(hypotheses, \n",
    "                    key=lambda h: (h.novelty_score, feasibility_order.get(h.feasibility, 0)), \n",
    "                    reverse=True)\n",
    "\n",
    "print(\"Top Research Hypotheses:\\n\")\n",
    "for i, hyp in enumerate(sorted_hyp[:10], 1):\n",
    "    print(f\"{i}. [Novelty: {hyp.novelty_score:.1f}/10, Feasibility: {hyp.feasibility}]\")\n",
    "    print(f\"   Statement: {hyp.statement}\")\n",
    "    print(f\"   Rationale: {hyp.rationale[:150]}...\")\n",
    "    if hyp.materials_involved:\n",
    "        print(f\"   Materials: {', '.join(hyp.materials_involved[:5])}\")\n",
    "    if hyp.suggested_methods:\n",
    "        print(f\"   Methods: {', '.join(hyp.suggested_methods[:3])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0ccc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis statistics\n",
    "hyp_feasibilities = Counter(h.feasibility for h in hypotheses)\n",
    "hyp_novelties = [h.novelty_score for h in hypotheses]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Feasibility distribution\n",
    "axes[0].bar(hyp_feasibilities.keys(), hyp_feasibilities.values())\n",
    "axes[0].set_title('Hypotheses by Feasibility', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Novelty distribution\n",
    "axes[1].hist(hyp_novelties, bins=10, edgecolor='black', alpha=0.7, color='purple')\n",
    "axes[1].set_title('Hypothesis Novelty Scores', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Novelty Score (0-10)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].axvline(np.mean(hyp_novelties), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(hyp_novelties):.2f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5bf07",
   "metadata": {},
   "source": [
    "## 8. Custom Queries\n",
    "\n",
    "Run custom queries on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b769e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find papers about a specific material\n",
    "material_of_interest = \"graphene\"  # Change this\n",
    "\n",
    "relevant_analyses = [\n",
    "    a for a in analyses \n",
    "    if any(material_of_interest.lower() in m.lower() for m in a.materials)\n",
    "]\n",
    "\n",
    "print(f\"Papers mentioning '{material_of_interest}': {len(relevant_analyses)}\\n\")\n",
    "\n",
    "for analysis in relevant_analyses[:5]:\n",
    "    print(f\"â€¢ {analysis.title[:70]}...\")\n",
    "    print(f\"  Relevance: {analysis.relevance_score:.1f}/10\")\n",
    "    print(f\"  Properties: {', '.join(analysis.properties[:5])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da758ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find papers studying a specific property\n",
    "property_of_interest = \"thermal conductivity\"  # Change this\n",
    "\n",
    "relevant_analyses = [\n",
    "    a for a in analyses \n",
    "    if any(property_of_interest.lower() in p.lower() for p in a.properties)\n",
    "]\n",
    "\n",
    "print(f\"Papers studying '{property_of_interest}': {len(relevant_analyses)}\\n\")\n",
    "\n",
    "for analysis in relevant_analyses[:5]:\n",
    "    print(f\"â€¢ {analysis.title[:70]}...\")\n",
    "    print(f\"  Materials: {', '.join(analysis.materials[:5])}\")\n",
    "    print(f\"  Methods: {', '.join(analysis.methods[:3])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964910fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find computational papers\n",
    "computational = [a for a in analyses if a.research_type == 'computational']\n",
    "\n",
    "print(f\"Computational Papers: {len(computational)}\\n\")\n",
    "\n",
    "for analysis in computational[:5]:\n",
    "    print(f\"â€¢ {analysis.title[:70]}...\")\n",
    "    print(f\"  Methods: {', '.join(analysis.methods[:5])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d23df5",
   "metadata": {},
   "source": [
    "## 9. Summary & Next Steps\n",
    "\n",
    "Key insights and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e8164",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“š Data Collection:\")\n",
    "print(f\"   â€¢ {len(papers)} papers collected from arXiv\")\n",
    "print(f\"   â€¢ Date range: {papers_df['published_date'].min()} to {papers_df['published_date'].max()}\")\n",
    "print(f\"   â€¢ {len(cat_counts)} unique categories\")\n",
    "\n",
    "print(f\"\\nðŸ¤– AI Analysis:\")\n",
    "print(f\"   â€¢ {len(analyses)} papers analyzed\")\n",
    "print(f\"   â€¢ Average relevance: {analyses_df['relevance_score'].mean():.2f}/10\")\n",
    "print(f\"   â€¢ High relevance (â‰¥7.0): {len([a for a in analyses if a.relevance_score >= 7.0])} papers\")\n",
    "\n",
    "print(f\"\\nðŸ§  Knowledge Extraction:\")\n",
    "print(f\"   â€¢ {stats['total_nodes']} entities in knowledge graph\")\n",
    "print(f\"   â€¢ {stats['num_materials']} materials, {stats['num_properties']} properties\")\n",
    "print(f\"   â€¢ {len(patterns['material_property_pairs'])} material-property relationships\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Research Opportunities:\")\n",
    "print(f\"   â€¢ {len(gaps)} research gaps identified\")\n",
    "print(f\"   â€¢ {len([g for g in gaps if g.priority == 'high'])} high-priority gaps\")\n",
    "print(f\"   â€¢ {len(hypotheses)} research hypotheses generated\")\n",
    "print(f\"   â€¢ {len([h for h in hypotheses if h.novelty_score >= 7.0])} highly novel hypotheses\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Recommended Next Steps:\")\n",
    "print(f\"   1. Review high-priority research gaps in detail\")\n",
    "print(f\"   2. Evaluate top hypotheses for experimental/computational feasibility\")\n",
    "print(f\"   3. Identify collaboration opportunities based on author networks\")\n",
    "print(f\"   4. Focus on understudied material-property combinations\")\n",
    "print(f\"   5. Consider computational approaches for materials with limited experimental data\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
